{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b192731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32382d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ef1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbe1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_korean = pd.read_csv(\"./korean.csv\", encoding=\"utf-8-sig\")\n",
    "df_eng = pd.read_csv(\"./english.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa4eae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how is the market is reaction to the newly rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the sales increase is faster than the previous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>then we will have to call the manufacturer and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sure i will make a call and double the volume ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shall we take a look at the issues we discusse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 번역문\n",
       "0  how is the market is reaction to the newly rel...\n",
       "1  the sales increase is faster than the previous...\n",
       "2  then we will have to call the manufacturer and...\n",
       "3  sure i will make a call and double the volume ...\n",
       "4  shall we take a look at the issues we discusse..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02bf321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_korean, df_eng], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff42a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요</td>\n",
       "      <td>how is the market is reaction to the newly rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>판매량이 지난번 제품보다 빠르게 늘고 있습니다</td>\n",
       "      <td>the sales increase is faster than the previous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요</td>\n",
       "      <td>then we will have to call the manufacturer and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>네 제가 연락해서 주문량을 2배로 늘리겠습니다</td>\n",
       "      <td>sure i will make a call and double the volume ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>지난 회의 마지막에 논의했던 안건을 다시 볼까요</td>\n",
       "      <td>shall we take a look at the issues we discusse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            원문  \\\n",
       "0   이번 신제품 출시에 대한 시장의 반응은 어떤가요   \n",
       "1    판매량이 지난번 제품보다 빠르게 늘고 있습니다   \n",
       "2  그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요   \n",
       "3    네 제가 연락해서 주문량을 2배로 늘리겠습니다   \n",
       "4   지난 회의 마지막에 논의했던 안건을 다시 볼까요   \n",
       "\n",
       "                                                 번역문  \n",
       "0  how is the market is reaction to the newly rel...  \n",
       "1  the sales increase is faster than the previous...  \n",
       "2  then we will have to call the manufacturer and...  \n",
       "3  sure i will make a call and double the volume ...  \n",
       "4  shall we take a look at the issues we discusse...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421b7d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   원문      100000 non-null  object\n",
      " 1   번역문     100000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bb0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13495576",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset= train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260d81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainset[:, 0]\n",
    "y_train = trainset[:, 1]\n",
    "X_val = valset[:, 0]\n",
    "y_val = valset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039fa812",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mecab(\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "def tokenizer_kor(text):\n",
    "    return m.morphs(text)\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a45272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_kor = {}\n",
    "word2idx_kor[\"PAD\"] = 0\n",
    "word2idx_kor[\"UNK\"] = 1\n",
    "word2idx_kor[\"<sos>\"] = 2\n",
    "word2idx_kor[\"<eos>\"] = 3\n",
    "word2idx_eng = {}\n",
    "word2idx_eng[\"PAD\"] = 0\n",
    "word2idx_eng[\"UNK\"] = 1\n",
    "word2idx_eng[\"<sos>\"] = 2\n",
    "word2idx_eng[\"<eos>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9445bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 4\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = tokenizer_kor(X_train[i])\n",
    "    for token in X_train[i]:\n",
    "        if token not in word2idx_kor.keys():\n",
    "            word2idx_kor[token] = count\n",
    "            count += 1\n",
    "    X_train[i] = [\"<sos>\"] + X_train[i] + [\"<eos>\"]\n",
    "    \n",
    "for i in range(len(X_val)):\n",
    "    X_val[i] = tokenizer_kor(X_val[i])\n",
    "    X_val[i] = [\"<sos>\"] + X_val[i] + [\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff31ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 4\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = tokenizer_eng(y_train[i])\n",
    "    for token in y_train[i]:\n",
    "        if token not in word2idx_eng.keys():\n",
    "            word2idx_eng[token] = count\n",
    "            count += 1\n",
    "    y_train[i] = [\"<sos>\"] + y_train[i] + [\"<eos>\"]\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    y_val[i] = tokenizer_eng(y_val[i])\n",
    "    y_val[i] = [\"<sos>\"] + y_val[i] + [\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c70df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_kor = {y:x for x,y in word2idx_kor.items()}\n",
    "idx2word_eng = {y:x for x,y in word2idx_eng.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f061d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"glove_kor.txt\"\n",
    "output_file = \"tmp.txt\"\n",
    "\n",
    "glove2word2vec(input_file, output_file)\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(output_file, binary=False)\n",
    "\n",
    "vocab_size_kor = len(word2idx_kor.keys())\n",
    "embedding_size_kor = 100\n",
    "weight_kor = np.zeros((vocab_size_kor, embedding_size_kor))\n",
    "for i in range(4, vocab_size_kor):\n",
    "    if idx2word_kor[i] in glove.key_to_index.keys():\n",
    "        weight_kor[i] = glove[idx2word_kor[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bba7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"glove_eng.txt\"\n",
    "output_file = \"tmp.txt\"\n",
    "\n",
    "glove2word2vec(input_file, output_file)\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(output_file, binary=False)\n",
    "\n",
    "vocab_size_eng = len(word2idx_eng.keys())\n",
    "embedding_size_eng = 100\n",
    "weight_eng = np.zeros((vocab_size_eng, embedding_size_eng))\n",
    "for i in range(4, vocab_size_eng):\n",
    "    if idx2word_eng[i] in glove.key_to_index.keys():\n",
    "        weight_eng[i] = glove[idx2word_eng[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d08f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2idx(data, word2idx):\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] in word2idx.keys():\n",
    "                data[i][j] = word2idx[data[i][j]]\n",
    "            else:\n",
    "                data[i][j] = word2idx[\"UNK\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55dc422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2idx(X_train, word2idx_kor)\n",
    "X_val = sent2idx(X_val, word2idx_kor)\n",
    "y_train = sent2idx(y_train, word2idx_eng)\n",
    "y_val = sent2idx(y_val, word2idx_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e2aab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(data, word2idx):\n",
    "    max_length = 0\n",
    "    length_list = []\n",
    "    \n",
    "    for i in data:\n",
    "        length_list.append(len(i))\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "            \n",
    "    for i in data:\n",
    "        for _ in range(max_length-len(i)):\n",
    "            i.append(word2idx[\"PAD\"])\n",
    "    \n",
    "    data = torch.tensor(data.tolist())\n",
    "    \n",
    "    return torch.tensor(data), length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75161203",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = make_tensor(X_train, word2idx_kor)\n",
    "X_val_tensor = make_tensor(X_val, word2idx_kor)\n",
    "y_train_tensor = make_tensor(y_train, word2idx_eng)\n",
    "y_val_tensor = make_tensor(y_val, word2idx_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91c457e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_tensor, y_tensor):\n",
    "        self.x = X_tensor[0]\n",
    "        self.x_l = X_tensor[1]\n",
    "        self.y = y_tensor[0]\n",
    "        self.y_l = y_tensor[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.x_l[index], self.y[index], self.y_l[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05427590",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train_tensor,  y_train_tensor)\n",
    "valset = CustomDataset(X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e7a21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0acc7575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 와 cuda 중 다음 기기로 학슴함:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu 와 cuda 중 다음 기기로 학슴함: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f529a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, length):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        packed_input = pack_padded_sequence(embedded, length.tolist(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output,(hidden, cell) = self.rnn(packed_input)\n",
    "        outputs, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72b89691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dimensions, attention_type='general'):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        if attention_type not in ['dot', 'general']:\n",
    "            raise ValueError('Invalid attention type selected.')\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        if self.attention_type == 'general':\n",
    "            self.linear_in = nn.Linear(dimensions, dimensions, bias=False)\n",
    "\n",
    "        self.linear_out = nn.Linear(dimensions * 2, dimensions, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, query, context):\n",
    "\n",
    "        batch_size, output_len, dimensions = query.size()\n",
    "        query_len = context.size(1)\n",
    "\n",
    "        if self.attention_type == \"general\":\n",
    "            query = query.reshape(batch_size * output_len, dimensions)\n",
    "            query = self.linear_in(query)\n",
    "            query = query.reshape(batch_size, output_len, dimensions)\n",
    "\n",
    "        # TODO: Include mask on PADDING_INDEX?\n",
    "\n",
    "        # (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, query_len)\n",
    "        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n",
    "\n",
    "        # Compute weights across every context sequence\n",
    "        attention_scores = attention_scores.view(batch_size * output_len, query_len)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        attention_weights = attention_weights.view(batch_size, output_len, query_len)\n",
    "\n",
    "        # (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, dimensions)\n",
    "        mix = torch.bmm(attention_weights, context)\n",
    "\n",
    "        # concat -> (batch_size * output_len, 2*dimensions)\n",
    "        combined = torch.cat((mix, query), dim=2)\n",
    "        combined = combined.view(batch_size * output_len, 2 * dimensions)\n",
    "\n",
    "        # Apply linear_out on every 2nd dimension of concat\n",
    "        # output -> (batch_size, output_len, dimensions)\n",
    "        output = self.linear_out(combined).view(batch_size, output_len, dimensions)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f3585eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout, batch_first = True)\n",
    "        \n",
    "        self.att = Attention(hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim*2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, en_outputs, hidden, cell):\n",
    "        \n",
    "        input = input.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        att_value, _ =  self.att(output, en_outputs)\n",
    "        \n",
    "        concat = torch.cat([output, att_value], dim=-1)\n",
    "        \n",
    "        prediction = self.fc_out(concat.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a042ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, src_length, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        en_outputs, hidden, cell = self.encoder(src, src_length)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, en_outputs, hidden, cell)\n",
    "            \n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8acb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word2idx_kor)\n",
    "OUTPUT_DIM = len(word2idx_eng)\n",
    "ENC_EMB_DIM = 100\n",
    "DEC_EMB_DIM = 100\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "enc.embedding.weight.data.copy_(torch.tensor(weight_kor))\n",
    "dec.embedding.weight.data.copy_(torch.tensor(weight_eng))\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f290cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(20472, 100)\n",
       "    (rnn): LSTM(100, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(16767, 100)\n",
       "    (rnn): LSTM(100, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (att): Attention(\n",
       "      (linear_in): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (fc_out): Linear(in_features=1024, out_features=16767, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6c5fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 28,413,947 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e007cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18656620",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = word2idx_eng[\"PAD\"]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1544073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch[0].to(device)\n",
    "        src_length = batch[1].to(device)\n",
    "        trg = batch[2].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_length, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f58a8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0].to(device)\n",
    "            src_length = batch[1].to(device)\n",
    "            trg = batch[2].to(device)\n",
    "\n",
    "            output = model(src, src_length, trg, 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d290204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29341f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 18s\n",
      "\tTrain Loss: 5.185 | Train PPL: 178.552\n",
      "\t Val. Loss: 5.273 |  Val. PPL: 195.069\n",
      "Epoch: 02 | Time: 9m 17s\n",
      "\tTrain Loss: 4.320 | Train PPL:  75.188\n",
      "\t Val. Loss: 4.935 |  Val. PPL: 139.133\n",
      "Epoch: 03 | Time: 9m 17s\n",
      "\tTrain Loss: 3.896 | Train PPL:  49.216\n",
      "\t Val. Loss: 4.817 |  Val. PPL: 123.642\n",
      "Epoch: 04 | Time: 9m 16s\n",
      "\tTrain Loss: 3.614 | Train PPL:  37.111\n",
      "\t Val. Loss: 4.725 |  Val. PPL: 112.764\n",
      "Epoch: 05 | Time: 9m 17s\n",
      "\tTrain Loss: 3.391 | Train PPL:  29.688\n",
      "\t Val. Loss: 4.640 |  Val. PPL: 103.540\n",
      "Epoch: 06 | Time: 9m 17s\n",
      "\tTrain Loss: 3.231 | Train PPL:  25.312\n",
      "\t Val. Loss: 4.631 |  Val. PPL: 102.586\n",
      "Epoch: 07 | Time: 9m 16s\n",
      "\tTrain Loss: 3.080 | Train PPL:  21.768\n",
      "\t Val. Loss: 4.742 |  Val. PPL: 114.679\n",
      "Epoch: 08 | Time: 9m 16s\n",
      "\tTrain Loss: 2.976 | Train PPL:  19.599\n",
      "\t Val. Loss: 4.648 |  Val. PPL: 104.409\n",
      "Epoch: 09 | Time: 9m 16s\n",
      "\tTrain Loss: 2.878 | Train PPL:  17.773\n",
      "\t Val. Loss: 4.728 |  Val. PPL: 113.100\n",
      "Epoch: 10 | Time: 9m 17s\n",
      "\tTrain Loss: 2.799 | Train PPL:  16.435\n",
      "\t Val. Loss: 4.671 |  Val. PPL: 106.844\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, trainloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq_with_att.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "757c9f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('seq2seq_with_att.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "683e24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, word2idx_kor, word2idx_eng, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    tokens = tokenizer_kor(sentence)\n",
    "    \n",
    "    src_indexes = [word2idx_kor[\"<sos>\"]] + [word2idx_kor[token] for token in tokens] + [word2idx_kor[\"<eos>\"]] \n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        en_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n",
    "        \n",
    "    trg_indexes = [word2idx_eng[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, en_outputs, hidden, cell)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == word2idx_eng[\"<eos>\"]:\n",
    "            break\n",
    "            \n",
    "    trg_tokens = [idx2word_eng[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4cc0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"내일 출장을 다녀와야 합니다\"\n",
    "result = translate_sentence(sentence, word2idx_kor, word2idx_eng, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "780ebfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'to', 'go', 'to', 'the', 'business', 'trip', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c33eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
