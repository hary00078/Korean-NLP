{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fe50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a91cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c94694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train_for_korean.csv\", encoding=\"utf-8-sig\")\n",
    "df_test = pd.read_csv(\"./test_for_korean.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e1521f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터 보고 초딩 영화 줄 오버 연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무 재밓었다 그래서 보는 것을 추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다 평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬 페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어 보이기만 했던 커스...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                    흠포스터 보고 초딩 영화 줄 오버 연기조차 가볍지 않구나      1\n",
       "2  10265843                             너무 재밓었다 그래서 보는 것을 추천한다      0\n",
       "3   9045019                         교도소 이야기구먼 솔직히 재미는 없다 평점 조정      0\n",
       "4   6483659  사이몬 페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어 보이기만 했던 커스...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f384800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(axis=0).reset_index(drop=True)\n",
    "space_idx = []\n",
    "for i in range(len(df_train)):\n",
    "    if str.isspace(df_train.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_train = df_train.drop(space_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38099cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.dropna(axis=0).reset_index(drop=True)\n",
    "space_idx = []\n",
    "for i in range(len(df_test)):\n",
    "    if str.isspace(df_test.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_test = df_test.drop(space_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab831662",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.array(df_train.drop([\"id\"], axis = 1))\n",
    "testset = np.array(df_test.drop([\"id\"], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b274a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset= train_test_split(trainset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eccea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainset[:, 0]\n",
    "y_train = trainset[:, 1]\n",
    "X_val = valset[:, 0]\n",
    "y_val = valset[:, 1]\n",
    "X_test = testset[:, 0]\n",
    "y_test = testset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d7428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(np.int64)\n",
    "y_val = y_val.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d28c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mecab(\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    return m.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "003110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "word2idx[\"PAD\"] = 0\n",
    "word2idx[\"UNK\"] = 1\n",
    "\n",
    "count = 2\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = tokenizer(X_train[i])\n",
    "    for token in X_train[i]:\n",
    "        if token not in word2idx.keys():\n",
    "            word2idx[token] = count\n",
    "            count += 1\n",
    "    \n",
    "for i in range(len(X_val)):\n",
    "    X_val[i] = tokenizer(X_val[i])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = tokenizer(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a8a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {y:x for x,y in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc3bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2idx(data, word2idx):\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] in word2idx.keys():\n",
    "                data[i][j] = word2idx[data[i][j]]\n",
    "            else:\n",
    "                data[i][j] = word2idx[\"UNK\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100070e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2idx(X_train, word2idx)\n",
    "X_val = sent2idx(X_val, word2idx)\n",
    "X_test = sent2idx(X_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b16d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"glove.txt\"\n",
    "output_file = \"tmp.txt\"\n",
    "\n",
    "glove2word2vec(input_file, output_file)\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3bf226",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx.keys())\n",
    "embedding_size = 100\n",
    "weight = np.zeros((vocab_size, embedding_size))\n",
    "for i in range(2, vocab_size):\n",
    "    if idx2word[i] in glove.key_to_index.keys():\n",
    "        weight[i] = glove[idx2word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "519116a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45716, 100])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.tensor(weight)\n",
    "print(weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a861a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(data, word2idx):\n",
    "    max_length = 0\n",
    "    length_list = []\n",
    "    \n",
    "    for i in data:\n",
    "        length_list.append(len(i))\n",
    "        if len(i) == 0:\n",
    "            print(\"ERROR\")\n",
    "            raise Exception()\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "            \n",
    "    for i in data:\n",
    "        for _ in range(max_length-len(i)):\n",
    "            i.append(word2idx[\"PAD\"])\n",
    "    \n",
    "    data = torch.tensor(data.tolist())\n",
    "    \n",
    "    return torch.tensor(data), length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9022c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_idx, X_train_length = make_tensor(X_train, word2idx)\n",
    "X_val_idx, X_val_length = make_tensor(X_val, word2idx)\n",
    "X_test_idx, X_test_length = make_tensor(X_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3095e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train)\n",
    "y_val = torch.tensor(y_val)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2892b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.unsqueeze(1)\n",
    "y_val = y_val.unsqueeze(1)\n",
    "y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1655848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, x_length, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.l = x_length\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.l[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8022c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train_idx, X_train_length, y_train)\n",
    "valset = CustomDataset(X_val_idx, X_val_length, y_val)\n",
    "testset = CustomDataset(X_test_idx, X_test_length, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1da75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d21257c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a75b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 와 cuda 중 다음 기기로 학슴함:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu 와 cuda 중 다음 기기로 학슴함: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4b5ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first= True,  bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_dim*2, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        embeded = self.embed(x)\n",
    "        packed_input = pack_padded_sequence(embeded, length.tolist(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output,(hidden, cell) = self.lstm(packed_input)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        logit = self.out(hidden)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b90d1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(3, 256, vocab_size, 100, n_classes).to(DEVICE)\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deab5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x , l, y = batch\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        y = y.reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x, l)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92eab1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x , l, y = batch\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.long().to(DEVICE)\n",
    "            y = y.reshape(-1)\n",
    "            logit = model(x, l)\n",
    "            loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()    \n",
    "            size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3321f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cdd5971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.4103,  0.1042,  0.2826,  ...,  0.3542,  0.8071, -1.5060],\n",
       "        ...,\n",
       "        [-0.3306, -0.0959,  0.5850,  ...,  0.3718, -0.2248, -0.1841],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2533,  0.4075, -0.0182,  ...,  0.3483, -0.0329, -0.1223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.data.copy_(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf1a3fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.435 | Train Acc: 79.52%\n",
      "\t Val. Loss: 0.393 |  Val. Acc: 82.27%\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.362 | Train Acc: 83.80%\n",
      "\t Val. Loss: 0.357 |  Val. Acc: 84.04%\n",
      "Epoch: 03 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.330 | Train Acc: 85.53%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 84.97%\n",
      "Epoch: 04 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.307 | Train Acc: 86.77%\n",
      "\t Val. Loss: 0.330 |  Val. Acc: 85.38%\n",
      "Epoch: 05 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.288 | Train Acc: 87.77%\n",
      "\t Val. Loss: 0.331 |  Val. Acc: 85.39%\n",
      "Epoch: 06 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.270 | Train Acc: 88.74%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 86.08%\n",
      "Epoch: 07 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.252 | Train Acc: 89.68%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 86.27%\n",
      "Epoch: 08 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.237 | Train Acc: 90.38%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 85.87%\n",
      "Epoch: 09 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.17%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 86.05%\n",
      "Epoch: 10 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.206 | Train Acc: 91.88%\n",
      "\t Val. Loss: 0.334 |  Val. Acc: 86.23%\n",
      "Epoch: 11 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.191 | Train Acc: 92.54%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 86.07%\n",
      "Epoch: 12 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.175 | Train Acc: 93.23%\n",
      "\t Val. Loss: 0.375 |  Val. Acc: 85.77%\n",
      "Epoch: 13 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.161 | Train Acc: 93.85%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 86.13%\n",
      "Epoch: 14 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.149 | Train Acc: 94.38%\n",
      "\t Val. Loss: 0.408 |  Val. Acc: 85.59%\n",
      "Epoch: 15 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.136 | Train Acc: 94.89%\n",
      "\t Val. Loss: 0.438 |  Val. Acc: 85.80%\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "n_epochs = 15\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, optimizer, trainloader)\n",
    "    val_loss, val_accuracy = evaluate(model, valloader)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"./textclassificatior.pt\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b28b397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./textclassificatior.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "266acae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(86.1040, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, testloader)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607681b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
