{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fe50cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c94694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train_for_korean.csv\", encoding=\"utf-8-sig\")\n",
    "df_test = pd.read_csv(\"./test_for_korean.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f384800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4308: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.dropna(axis=0)\n",
    "space_idx = []\n",
    "for i in range(len(df_train)):\n",
    "    if str.isspace(df_train.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_train = df_train.drop(df_train.index[[space_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38099cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4308: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.dropna(axis=0)\n",
    "space_idx = []\n",
    "for i in range(len(df_test)):\n",
    "    if str.isspace(df_test.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_test = df_test.drop(df_test.index[[space_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab831662",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.array(df_train.drop([\"id\"], axis = 1))\n",
    "testset = np.array(df_test.drop([\"id\"], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b274a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset= train_test_split(trainset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eccea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainset[:, 0]\n",
    "y_train = trainset[:, 1]\n",
    "X_val = valset[:, 0]\n",
    "y_val = valset[:, 1]\n",
    "X_test = testset[:, 0]\n",
    "y_test = testset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d7428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(np.long)\n",
    "y_val = y_val.astype(np.long)\n",
    "y_test = y_test.astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d28c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mecab(\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    return m.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "003110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "word2idx[\"PAD\"] = 0\n",
    "word2idx[\"UNK\"] = 1\n",
    "\n",
    "count = 2\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = tokenizer(X_train[i])\n",
    "    for token in X_train[i]:\n",
    "        if token not in word2idx.keys():\n",
    "            word2idx[token] = count\n",
    "            count += 1\n",
    "    \n",
    "for i in range(len(X_val)):\n",
    "    X_val[i] = tokenizer(X_val[i])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = tokenizer(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a8a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {y:x for x,y in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc3bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2idx(data, word2idx):\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] in word2idx.keys():\n",
    "                data[i][j] = word2idx[data[i][j]]\n",
    "            else:\n",
    "                data[i][j] = word2idx[\"UNK\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100070e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2idx(X_train, word2idx)\n",
    "X_val = sent2idx(X_val, word2idx)\n",
    "X_test = sent2idx(X_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b16d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"glove.txt\"\n",
    "output_file = \"tmp.txt\"\n",
    "\n",
    "glove2word2vec(input_file, output_file)\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b3bf226",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx.keys())\n",
    "embedding_size = 100\n",
    "weight = np.zeros((vocab_size, embedding_size))\n",
    "for i in range(2, vocab_size):\n",
    "    if idx2word[i] in glove.key_to_index.keys():\n",
    "        weight[i] = glove[idx2word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "519116a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45747, 100])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.tensor(weight)\n",
    "print(weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a861a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(data, word2idx):\n",
    "    max_length = 0\n",
    "    length_list = []\n",
    "    \n",
    "    for i in data:\n",
    "        length_list.append(len(i))\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "            \n",
    "    for i in data:\n",
    "        for _ in range(max_length-len(i)):\n",
    "            i.append(word2idx[\"PAD\"])\n",
    "    \n",
    "    data = torch.tensor(data.tolist())\n",
    "    \n",
    "    return torch.tensor(data), length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9022c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\konlpy\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "X_train_idx, X_train_length = make_tensor(X_train, word2idx)\n",
    "X_val_idx, X_val_length = make_tensor(X_val, word2idx)\n",
    "X_test_idx, X_test_length = make_tensor(X_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3095e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train)\n",
    "y_val = torch.tensor(y_val)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2892b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.unsqueeze(1)\n",
    "y_val = y_val.unsqueeze(1)\n",
    "y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1655848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, x_length, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.l = x_length\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.l[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8022c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train_idx, X_train_length, y_train)\n",
    "valset = CustomDataset(X_val_idx, X_val_length, y_val)\n",
    "testset = CustomDataset(X_test_idx, X_test_length, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f1da75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d21257c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a75b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 와 cuda 중 다음 기기로 학슴함:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu 와 cuda 중 다음 기기로 학슴함: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4b5ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first= True,  bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_dim*2, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        embeded = self.embed(x)\n",
    "        packed_input = pack_padded_sequence(embeded, length.tolist(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output,(hidden, cell) = self.lstm(packed_input)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        logit = self.out(hidden)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b90d1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(3, 256, vocab_size, 100, n_classes).to(DEVICE)\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deab5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x , l, y = batch\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        y = y.reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x, l)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92eab1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x , l, y = batch\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.long().to(DEVICE)\n",
    "            y = y.reshape(-1)\n",
    "            logit = model(x, l)\n",
    "            loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()    \n",
    "            size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cdd5971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0527, -0.1771,  0.5961,  ..., -0.1474,  1.0186, -0.8482],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0792,  0.1054, -0.2989,  ...,  0.0477,  0.2381,  0.1649],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.data.copy_(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1a3fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] val loss :  0.38 | val acuuracy : 82.59\n",
      "[Epoch: 0] train loss :  0.44 | train acuuracy : 79.58\n",
      "[Epoch: 1] val loss :  0.35 | val acuuracy : 84.72\n",
      "[Epoch: 1] train loss :  0.36 | train acuuracy : 83.77\n",
      "[Epoch: 2] val loss :  0.33 | val acuuracy : 85.60\n",
      "[Epoch: 2] train loss :  0.33 | train acuuracy : 85.43\n",
      "[Epoch: 3] val loss :  0.32 | val acuuracy : 85.88\n",
      "[Epoch: 3] train loss :  0.31 | train acuuracy : 86.67\n",
      "[Epoch: 4] val loss :  0.31 | val acuuracy : 86.37\n",
      "[Epoch: 4] train loss :  0.29 | train acuuracy : 87.83\n",
      "[Epoch: 5] val loss :  0.32 | val acuuracy : 86.26\n",
      "[Epoch: 5] train loss :  0.27 | train acuuracy : 88.77\n",
      "[Epoch: 6] val loss :  0.32 | val acuuracy : 86.36\n",
      "[Epoch: 6] train loss :  0.25 | train acuuracy : 89.67\n",
      "[Epoch: 7] val loss :  0.33 | val acuuracy : 86.10\n",
      "[Epoch: 7] train loss :  0.24 | train acuuracy : 90.45\n",
      "[Epoch: 8] val loss :  0.32 | val acuuracy : 86.73\n",
      "[Epoch: 8] train loss :  0.22 | train acuuracy : 91.13\n",
      "[Epoch: 9] val loss :  0.34 | val acuuracy : 86.86\n",
      "[Epoch: 9] train loss :  0.21 | train acuuracy : 91.72\n",
      "[Epoch: 10] val loss :  0.34 | val acuuracy : 86.56\n",
      "[Epoch: 10] train loss :  0.19 | train acuuracy : 92.46\n",
      "[Epoch: 11] val loss :  0.37 | val acuuracy : 86.15\n",
      "[Epoch: 11] train loss :  0.18 | train acuuracy : 93.04\n",
      "[Epoch: 12] val loss :  0.38 | val acuuracy : 86.11\n",
      "[Epoch: 12] train loss :  0.16 | train acuuracy : 93.68\n",
      "[Epoch: 13] val loss :  0.42 | val acuuracy : 86.17\n",
      "[Epoch: 13] train loss :  0.15 | train acuuracy : 94.19\n",
      "[Epoch: 14] val loss :  0.47 | val acuuracy : 84.92\n",
      "[Epoch: 14] train loss :  0.14 | train acuuracy : 94.71\n",
      "[Epoch: 15] val loss :  0.51 | val acuuracy : 85.39\n",
      "[Epoch: 15] train loss :  0.13 | train acuuracy : 95.11\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "n_epochs = 15\n",
    "for epoch in range(n_epochs+1):\n",
    "    train_loss, train_accuracy = train(model, optimizer, trainloader)\n",
    "    val_loss, val_accuracy = evaluate(model, valloader)\n",
    "    \n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val acuuracy : %5.2f\" % (epoch, val_loss, val_accuracy))\n",
    "    print(\"[Epoch: %d] train loss : %5.2f | train acuuracy : %5.2f\" % (epoch, train_loss, train_accuracy))\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"./textclassificatior.pt\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b28b397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./textclassificatior.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "266acae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(86.1101, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, testloader)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607681b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
