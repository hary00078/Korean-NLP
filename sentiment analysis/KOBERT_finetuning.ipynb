{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b6611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb04bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train_for_korean.csv\", encoding=\"utf-8-sig\")\n",
    "df_test = pd.read_csv(\"./test_for_korean.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b075f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4307: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.dropna(axis=0)\n",
    "space_idx = []\n",
    "for i in range(len(df_train)):\n",
    "    if str.isspace(df_train.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_train = df_train.drop(df_train.index[[space_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90da1c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4307: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.dropna(axis=0)\n",
    "space_idx = []\n",
    "for i in range(len(df_test)):\n",
    "    if str.isspace(df_test.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_test = df_test.drop(df_test.index[[space_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11addd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.array(df_train.drop([\"id\"], axis = 1))\n",
    "testset = np.array(df_test.drop([\"id\"], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2e8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset= train_test_split(trainset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2472a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainset[:, 0]\n",
    "y_train = trainset[:, 1]\n",
    "X_val = valset[:, 0]\n",
    "y_val = valset[:, 1]\n",
    "X_test = testset[:, 0]\n",
    "y_test = testset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe4ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(np.long)\n",
    "y_val = y_val.astype(np.long)\n",
    "y_test = y_test.astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd8b552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08c31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a701a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "transform = nlp.data.BERTSentenceTransform(\n",
    "            tok, max_seq_length=max_len, pad=True, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6510a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.idx = torch.tensor([transform([sentence])[0] for sentence in x]).reshape(len(x), -1)\n",
    "        self.l = torch.tensor([transform([sentence])[1].item() for sentence in x])\n",
    "        self.s = torch.tensor([transform([sentence])[2] for sentence in x]).reshape(len(x), -1)\n",
    "        self.y = torch.tensor(y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.idx[index], self.l[index], self.s[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "824e10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train, y_train, transform)\n",
    "valset = CustomDataset(X_val, y_val, transform)\n",
    "testset = CustomDataset(X_test, y_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4387449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8f01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c9d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 와 cuda 중 다음 기기로 학슴함:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu 와 cuda 중 다음 기기로 학슴함: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87761ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 hidden_size = 768):\n",
    "        \n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "            self.dr_rate = dr_rate\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        output = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float())\n",
    "        output = output[1]\n",
    "        if self.dr_rate:\n",
    "            output = self.dropout(output)\n",
    "        return self.classifier(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca326bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.1).to(DEVICE)\n",
    "lr = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e89471c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x , l, s, y = batch\n",
    "        x = x.to(DEVICE)\n",
    "        l = l.to(DEVICE)\n",
    "        s = s.to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        y = y.reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x, l, s)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cad6f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x , l, s, y = batch\n",
    "            x = x.to(DEVICE)\n",
    "            l = l.to(DEVICE)\n",
    "            s = s.to(DEVICE)\n",
    "            y = y.long().to(DEVICE)\n",
    "            y = y.reshape(-1)\n",
    "            logit = model(x, l, s)\n",
    "            loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()    \n",
    "            size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "861fe930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] val loss :  0.30 | val acuuracy : 87.97\n",
      "[Epoch: 0] train loss :  0.34 | train acuuracy : 85.08\n",
      "[Epoch: 1] val loss :  0.28 | val acuuracy : 88.55\n",
      "[Epoch: 1] train loss :  0.25 | train acuuracy : 89.60\n",
      "[Epoch: 2] val loss :  0.31 | val acuuracy : 88.35\n",
      "[Epoch: 2] train loss :  0.20 | train acuuracy : 91.95\n",
      "[Epoch: 3] val loss :  0.32 | val acuuracy : 88.31\n",
      "[Epoch: 3] train loss :  0.15 | train acuuracy : 94.30\n",
      "[Epoch: 4] val loss :  0.38 | val acuuracy : 87.75\n",
      "[Epoch: 4] train loss :  0.12 | train acuuracy : 95.57\n",
      "[Epoch: 5] val loss :  0.39 | val acuuracy : 87.93\n",
      "[Epoch: 5] train loss :  0.10 | train acuuracy : 96.59\n",
      "[Epoch: 6] val loss :  0.44 | val acuuracy : 88.20\n",
      "[Epoch: 6] train loss :  0.08 | train acuuracy : 97.26\n",
      "[Epoch: 7] val loss :  0.42 | val acuuracy : 88.15\n",
      "[Epoch: 7] train loss :  0.07 | train acuuracy : 97.62\n",
      "[Epoch: 8] val loss :  0.44 | val acuuracy : 87.92\n",
      "[Epoch: 8] train loss :  0.06 | train acuuracy : 97.76\n",
      "[Epoch: 9] val loss :  0.44 | val acuuracy : 87.95\n",
      "[Epoch: 9] train loss :  0.06 | train acuuracy : 97.99\n",
      "[Epoch: 10] val loss :  0.55 | val acuuracy : 87.65\n",
      "[Epoch: 10] train loss :  0.05 | train acuuracy : 98.16\n",
      "[Epoch: 11] val loss :  0.54 | val acuuracy : 88.22\n",
      "[Epoch: 11] train loss :  0.05 | train acuuracy : 98.27\n",
      "[Epoch: 12] val loss :  0.54 | val acuuracy : 87.48\n",
      "[Epoch: 12] train loss :  0.05 | train acuuracy : 98.35\n",
      "[Epoch: 13] val loss :  0.50 | val acuuracy : 87.93\n",
      "[Epoch: 13] train loss :  0.04 | train acuuracy : 98.44\n",
      "[Epoch: 14] val loss :  0.56 | val acuuracy : 88.05\n",
      "[Epoch: 14] train loss :  0.04 | train acuuracy : 98.47\n",
      "[Epoch: 15] val loss :  0.54 | val acuuracy : 87.74\n",
      "[Epoch: 15] train loss :  0.04 | train acuuracy : 98.56\n",
      "[Epoch: 16] val loss :  0.60 | val acuuracy : 87.56\n",
      "[Epoch: 16] train loss :  0.04 | train acuuracy : 98.65\n",
      "[Epoch: 17] val loss :  0.56 | val acuuracy : 87.38\n",
      "[Epoch: 17] train loss :  0.04 | train acuuracy : 98.60\n",
      "[Epoch: 18] val loss :  0.50 | val acuuracy : 87.67\n",
      "[Epoch: 18] train loss :  0.04 | train acuuracy : 98.68\n",
      "[Epoch: 19] val loss :  0.57 | val acuuracy : 87.49\n",
      "[Epoch: 19] train loss :  0.04 | train acuuracy : 98.73\n",
      "[Epoch: 20] val loss :  0.53 | val acuuracy : 87.74\n",
      "[Epoch: 20] train loss :  0.04 | train acuuracy : 98.77\n",
      "[Epoch: 21] val loss :  0.57 | val acuuracy : 87.72\n",
      "[Epoch: 21] train loss :  0.04 | train acuuracy : 98.73\n",
      "[Epoch: 22] val loss :  0.55 | val acuuracy : 87.46\n",
      "[Epoch: 22] train loss :  0.03 | train acuuracy : 98.78\n",
      "[Epoch: 23] val loss :  0.59 | val acuuracy : 87.67\n",
      "[Epoch: 23] train loss :  0.04 | train acuuracy : 98.75\n",
      "[Epoch: 24] val loss :  0.58 | val acuuracy : 87.88\n",
      "[Epoch: 24] train loss :  0.03 | train acuuracy : 98.92\n",
      "[Epoch: 25] val loss :  0.57 | val acuuracy : 87.34\n",
      "[Epoch: 25] train loss :  0.03 | train acuuracy : 98.87\n",
      "[Epoch: 26] val loss :  0.56 | val acuuracy : 88.05\n",
      "[Epoch: 26] train loss :  0.03 | train acuuracy : 98.88\n",
      "[Epoch: 27] val loss :  0.57 | val acuuracy : 87.99\n",
      "[Epoch: 27] train loss :  0.03 | train acuuracy : 98.89\n",
      "[Epoch: 28] val loss :  0.63 | val acuuracy : 87.91\n",
      "[Epoch: 28] train loss :  0.04 | train acuuracy : 98.67\n",
      "[Epoch: 29] val loss :  0.55 | val acuuracy : 87.74\n",
      "[Epoch: 29] train loss :  0.03 | train acuuracy : 98.97\n",
      "[Epoch: 30] val loss :  0.61 | val acuuracy : 87.19\n",
      "[Epoch: 30] train loss :  0.03 | train acuuracy : 98.94\n",
      "[Epoch: 31] val loss :  0.60 | val acuuracy : 87.99\n",
      "[Epoch: 31] train loss :  0.03 | train acuuracy : 98.96\n",
      "[Epoch: 32] val loss :  0.58 | val acuuracy : 87.76\n",
      "[Epoch: 32] train loss :  0.03 | train acuuracy : 98.91\n",
      "[Epoch: 33] val loss :  0.58 | val acuuracy : 87.46\n",
      "[Epoch: 33] train loss :  0.03 | train acuuracy : 98.94\n",
      "[Epoch: 34] val loss :  0.56 | val acuuracy : 87.41\n",
      "[Epoch: 34] train loss :  0.03 | train acuuracy : 98.99\n",
      "[Epoch: 35] val loss :  0.65 | val acuuracy : 87.80\n",
      "[Epoch: 35] train loss :  0.03 | train acuuracy : 99.01\n",
      "[Epoch: 36] val loss :  0.63 | val acuuracy : 87.62\n",
      "[Epoch: 36] train loss :  0.03 | train acuuracy : 98.99\n",
      "[Epoch: 37] val loss :  0.57 | val acuuracy : 87.74\n",
      "[Epoch: 37] train loss :  0.03 | train acuuracy : 98.96\n",
      "[Epoch: 38] val loss :  0.53 | val acuuracy : 87.68\n",
      "[Epoch: 38] train loss :  0.03 | train acuuracy : 99.04\n",
      "[Epoch: 39] val loss :  0.63 | val acuuracy : 87.80\n",
      "[Epoch: 39] train loss :  0.03 | train acuuracy : 98.98\n",
      "[Epoch: 40] val loss :  0.65 | val acuuracy : 87.58\n",
      "[Epoch: 40] train loss :  0.03 | train acuuracy : 99.03\n",
      "[Epoch: 41] val loss :  0.57 | val acuuracy : 87.65\n",
      "[Epoch: 41] train loss :  0.03 | train acuuracy : 98.99\n",
      "[Epoch: 42] val loss :  0.70 | val acuuracy : 87.66\n",
      "[Epoch: 42] train loss :  0.03 | train acuuracy : 99.01\n",
      "[Epoch: 43] val loss :  0.65 | val acuuracy : 87.62\n",
      "[Epoch: 43] train loss :  0.03 | train acuuracy : 99.09\n",
      "[Epoch: 44] val loss :  0.67 | val acuuracy : 87.45\n",
      "[Epoch: 44] train loss :  0.03 | train acuuracy : 99.10\n",
      "[Epoch: 45] val loss :  0.55 | val acuuracy : 87.68\n",
      "[Epoch: 45] train loss :  0.03 | train acuuracy : 99.03\n",
      "[Epoch: 46] val loss :  0.65 | val acuuracy : 87.53\n",
      "[Epoch: 46] train loss :  0.03 | train acuuracy : 99.12\n",
      "[Epoch: 47] val loss :  0.62 | val acuuracy : 87.65\n",
      "[Epoch: 47] train loss :  0.03 | train acuuracy : 99.11\n",
      "[Epoch: 48] val loss :  0.63 | val acuuracy : 87.05\n",
      "[Epoch: 48] train loss :  0.03 | train acuuracy : 99.05\n",
      "[Epoch: 49] val loss :  0.57 | val acuuracy : 87.74\n",
      "[Epoch: 49] train loss :  0.03 | train acuuracy : 99.12\n",
      "[Epoch: 50] val loss :  0.62 | val acuuracy : 87.95\n",
      "[Epoch: 50] train loss :  0.03 | train acuuracy : 99.02\n",
      "[Epoch: 51] val loss :  0.65 | val acuuracy : 87.80\n",
      "[Epoch: 51] train loss :  0.02 | train acuuracy : 99.16\n",
      "[Epoch: 52] val loss :  0.61 | val acuuracy : 87.45\n",
      "[Epoch: 52] train loss :  0.03 | train acuuracy : 98.99\n",
      "[Epoch: 53] val loss :  0.60 | val acuuracy : 87.72\n",
      "[Epoch: 53] train loss :  0.03 | train acuuracy : 98.96\n",
      "[Epoch: 54] val loss :  0.63 | val acuuracy : 87.85\n",
      "[Epoch: 54] train loss :  0.03 | train acuuracy : 99.15\n",
      "[Epoch: 55] val loss :  0.62 | val acuuracy : 87.87\n",
      "[Epoch: 55] train loss :  0.03 | train acuuracy : 98.92\n",
      "[Epoch: 56] val loss :  0.63 | val acuuracy : 87.50\n",
      "[Epoch: 56] train loss :  0.03 | train acuuracy : 99.05\n",
      "[Epoch: 57] val loss :  0.63 | val acuuracy : 87.80\n",
      "[Epoch: 57] train loss :  0.03 | train acuuracy : 99.11\n",
      "[Epoch: 58] val loss :  0.57 | val acuuracy : 87.20\n",
      "[Epoch: 58] train loss :  0.03 | train acuuracy : 99.13\n",
      "[Epoch: 59] val loss :  0.64 | val acuuracy : 87.29\n",
      "[Epoch: 59] train loss :  0.03 | train acuuracy : 99.10\n",
      "[Epoch: 60] val loss :  0.54 | val acuuracy : 87.26\n",
      "[Epoch: 60] train loss :  0.08 | train acuuracy : 97.29\n",
      "[Epoch: 61] val loss :  0.66 | val acuuracy : 87.60\n",
      "[Epoch: 61] train loss :  0.03 | train acuuracy : 98.95\n",
      "[Epoch: 62] val loss :  0.64 | val acuuracy : 87.92\n",
      "[Epoch: 62] train loss :  0.02 | train acuuracy : 99.31\n",
      "[Epoch: 63] val loss :  0.71 | val acuuracy : 87.74\n",
      "[Epoch: 63] train loss :  0.02 | train acuuracy : 99.25\n",
      "[Epoch: 64] val loss :  0.62 | val acuuracy : 87.07\n",
      "[Epoch: 64] train loss :  0.02 | train acuuracy : 99.19\n",
      "[Epoch: 65] val loss :  0.64 | val acuuracy : 87.57\n",
      "[Epoch: 65] train loss :  0.03 | train acuuracy : 99.11\n",
      "[Epoch: 66] val loss :  0.67 | val acuuracy : 87.65\n",
      "[Epoch: 66] train loss :  0.03 | train acuuracy : 99.08\n",
      "[Epoch: 67] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 67] train loss :  0.14 | train acuuracy : 91.29\n",
      "[Epoch: 68] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 68] train loss :  0.70 | train acuuracy : 49.96\n",
      "[Epoch: 69] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 69] train loss :  0.69 | train acuuracy : 50.12\n",
      "[Epoch: 70] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 70] train loss :  0.69 | train acuuracy : 50.18\n",
      "[Epoch: 71] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 71] train loss :  0.69 | train acuuracy : 50.02\n",
      "[Epoch: 72] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 72] train loss :  0.69 | train acuuracy : 49.84\n",
      "[Epoch: 73] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 73] train loss :  0.69 | train acuuracy : 50.10\n",
      "[Epoch: 74] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 74] train loss :  0.69 | train acuuracy : 50.05\n",
      "[Epoch: 75] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 75] train loss :  0.69 | train acuuracy : 50.06\n",
      "[Epoch: 76] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 76] train loss :  0.69 | train acuuracy : 49.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 77] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 77] train loss :  0.69 | train acuuracy : 49.99\n",
      "[Epoch: 78] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 78] train loss :  0.69 | train acuuracy : 50.12\n",
      "[Epoch: 79] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 79] train loss :  0.69 | train acuuracy : 50.11\n",
      "[Epoch: 80] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 80] train loss :  0.69 | train acuuracy : 50.10\n",
      "[Epoch: 81] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 81] train loss :  0.69 | train acuuracy : 50.06\n",
      "[Epoch: 82] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 82] train loss :  0.69 | train acuuracy : 50.01\n",
      "[Epoch: 83] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 83] train loss :  0.69 | train acuuracy : 50.05\n",
      "[Epoch: 84] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 84] train loss :  0.69 | train acuuracy : 50.05\n",
      "[Epoch: 85] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 85] train loss :  0.69 | train acuuracy : 49.98\n",
      "[Epoch: 86] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 86] train loss :  0.69 | train acuuracy : 50.14\n",
      "[Epoch: 87] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 87] train loss :  0.69 | train acuuracy : 50.09\n",
      "[Epoch: 88] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 88] train loss :  0.69 | train acuuracy : 50.24\n",
      "[Epoch: 89] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 89] train loss :  0.69 | train acuuracy : 50.08\n",
      "[Epoch: 90] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 90] train loss :  0.69 | train acuuracy : 50.48\n",
      "[Epoch: 91] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 91] train loss :  0.69 | train acuuracy : 50.21\n",
      "[Epoch: 92] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 92] train loss :  0.69 | train acuuracy : 50.03\n",
      "[Epoch: 93] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 93] train loss :  0.69 | train acuuracy : 49.79\n",
      "[Epoch: 94] val loss :  0.69 | val acuuracy : 50.36\n",
      "[Epoch: 94] train loss :  0.69 | train acuuracy : 49.85\n",
      "[Epoch: 95] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 95] train loss :  0.69 | train acuuracy : 50.09\n",
      "[Epoch: 96] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 96] train loss :  0.69 | train acuuracy : 50.19\n",
      "[Epoch: 97] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 97] train loss :  0.69 | train acuuracy : 50.23\n",
      "[Epoch: 98] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 98] train loss :  0.69 | train acuuracy : 50.14\n",
      "[Epoch: 99] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 99] train loss :  0.69 | train acuuracy : 50.12\n",
      "[Epoch: 100] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 100] train loss :  0.69 | train acuuracy : 50.23\n",
      "[Epoch: 101] val loss :  0.69 | val acuuracy : 49.64\n",
      "[Epoch: 101] train loss :  0.69 | train acuuracy : 50.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36464/1130732765.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36464/2054976731.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcorrects\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs+1):\n",
    "    train_loss, train_accuracy = train(model, optimizer, trainloader)\n",
    "    val_loss, val_accuracy = evaluate(model, valloader)\n",
    "    \n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val acuuracy : %5.2f\" % (epoch, val_loss, val_accuracy))\n",
    "    print(\"[Epoch: %d] train loss : %5.2f | train acuuracy : %5.2f\" % (epoch, train_loss, train_accuracy))\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        model.bert.save_pretrained(\"kobert_weight\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0fefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
