{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2243ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a539562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train_for_korean.csv\", encoding=\"utf-8-sig\")\n",
    "df_test = pd.read_csv(\"./test_for_korean.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544f3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4307: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.dropna(axis=0)\n",
    "space_idx = []\n",
    "for i in range(len(df_train)):\n",
    "    if str.isspace(df_train.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_train = df_train.drop(df_train.index[[space_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bb50ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4307: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.dropna(axis=0)\n",
    "space_idx = []\n",
    "for i in range(len(df_test)):\n",
    "    if str.isspace(df_test.iloc[i, 1]) == True:\n",
    "        space_idx.append(i)\n",
    "df_test = df_test.drop(df_test.index[[space_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a93733",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.array(df_train.drop([\"id\"], axis = 1))\n",
    "testset = np.array(df_test.drop([\"id\"], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17470655",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset= train_test_split(trainset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29570535",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainset[:, 0]\n",
    "y_train = trainset[:, 1]\n",
    "X_val = valset[:, 0]\n",
    "y_val = valset[:, 1]\n",
    "X_test = testset[:, 0]\n",
    "y_test = testset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d24d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\Users\\user\\anaconda3\\envs\\kobert\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(np.long)\n",
    "y_val = y_val.astype(np.long)\n",
    "y_test = y_test.astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af049921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "bertmodel = bertmodel.from_pretrained(\"kobert_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f82e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1d1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "transform = nlp.data.BERTSentenceTransform(\n",
    "            tok, max_seq_length=max_len, pad=True, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adde35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.idx = torch.tensor([transform([sentence])[0] for sentence in x]).reshape(len(x), -1)\n",
    "        self.l = torch.tensor([transform([sentence])[1].item() for sentence in x])\n",
    "        self.s = torch.tensor([transform([sentence])[2] for sentence in x]).reshape(len(x), -1)\n",
    "        self.y = torch.tensor(y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.idx[index], self.l[index], self.s[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43537ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train, y_train, transform)\n",
    "valset = CustomDataset(X_val, y_val, transform)\n",
    "testset = CustomDataset(X_test, y_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054218b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee083d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e21684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 와 cuda 중 다음 기기로 학슴함:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu 와 cuda 중 다음 기기로 학슴함: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32cce681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 bert_size = 768,\n",
    "                 hidden_size = 256,\n",
    "                 n_layers = 2,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        self.lstm = nn.LSTM(bert_size, hidden_size, num_layers=n_layers, batch_first= True,  bidirectional=True)\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size*2 , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeded = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        \n",
    "        if self.dr_rate:\n",
    "            embeded = self.dropout(embeded[0])\n",
    "        else:\n",
    "            embeded = embeded[0]\n",
    "            \n",
    "        packed_input = pack_padded_sequence(embeded, valid_length.tolist(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output,(hidden, cell) = self.lstm(packed_input)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        logit = self.classifier(hidden)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c663fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel).to(DEVICE)\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d2e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdc0d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x , l, s, y = batch\n",
    "        x = x.to(DEVICE)\n",
    "        l = l.to(DEVICE)\n",
    "        s = s.to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        y = y.reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x, l, s)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7530eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x , l, s, y = batch\n",
    "            x = x.to(DEVICE)\n",
    "            l = l.to(DEVICE)\n",
    "            s = s.to(DEVICE)\n",
    "            y = y.long().to(DEVICE)\n",
    "            y = y.reshape(-1)\n",
    "            logit = model(x, l, s)\n",
    "            loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()    \n",
    "            size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51a09216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] val loss :  0.17 | val acuuracy : 93.79\n",
      "[Epoch: 0] train loss :  0.19 | train acuuracy : 92.66\n",
      "[Epoch: 1] val loss :  0.17 | val acuuracy : 93.83\n",
      "[Epoch: 1] train loss :  0.18 | train acuuracy : 92.91\n",
      "[Epoch: 2] val loss :  0.16 | val acuuracy : 93.81\n",
      "[Epoch: 2] train loss :  0.18 | train acuuracy : 92.98\n",
      "[Epoch: 3] val loss :  0.16 | val acuuracy : 93.86\n",
      "[Epoch: 3] train loss :  0.18 | train acuuracy : 93.07\n",
      "[Epoch: 4] val loss :  0.16 | val acuuracy : 93.81\n",
      "[Epoch: 4] train loss :  0.18 | train acuuracy : 93.11\n",
      "[Epoch: 5] val loss :  0.17 | val acuuracy : 93.61\n",
      "[Epoch: 5] train loss :  0.18 | train acuuracy : 93.13\n",
      "[Epoch: 6] val loss :  0.17 | val acuuracy : 93.67\n",
      "[Epoch: 6] train loss :  0.18 | train acuuracy : 93.20\n",
      "[Epoch: 7] val loss :  0.17 | val acuuracy : 93.41\n",
      "[Epoch: 7] train loss :  0.18 | train acuuracy : 93.20\n",
      "[Epoch: 8] val loss :  0.16 | val acuuracy : 93.91\n",
      "[Epoch: 8] train loss :  0.17 | train acuuracy : 93.36\n",
      "[Epoch: 9] val loss :  0.17 | val acuuracy : 93.67\n",
      "[Epoch: 9] train loss :  0.17 | train acuuracy : 93.33\n",
      "[Epoch: 10] val loss :  0.17 | val acuuracy : 93.69\n",
      "[Epoch: 10] train loss :  0.17 | train acuuracy : 93.41\n",
      "[Epoch: 11] val loss :  0.17 | val acuuracy : 93.50\n",
      "[Epoch: 11] train loss :  0.17 | train acuuracy : 93.48\n",
      "[Epoch: 12] val loss :  0.17 | val acuuracy : 93.72\n",
      "[Epoch: 12] train loss :  0.17 | train acuuracy : 93.54\n",
      "[Epoch: 13] val loss :  0.17 | val acuuracy : 93.76\n",
      "[Epoch: 13] train loss :  0.17 | train acuuracy : 93.61\n",
      "[Epoch: 14] val loss :  0.17 | val acuuracy : 93.80\n",
      "[Epoch: 14] train loss :  0.17 | train acuuracy : 93.64\n",
      "[Epoch: 15] val loss :  0.17 | val acuuracy : 93.80\n",
      "[Epoch: 15] train loss :  0.16 | train acuuracy : 93.80\n",
      "[Epoch: 16] val loss :  0.17 | val acuuracy : 93.67\n",
      "[Epoch: 16] train loss :  0.16 | train acuuracy : 93.80\n",
      "[Epoch: 17] val loss :  0.17 | val acuuracy : 93.50\n",
      "[Epoch: 17] train loss :  0.16 | train acuuracy : 93.94\n",
      "[Epoch: 18] val loss :  0.17 | val acuuracy : 93.55\n",
      "[Epoch: 18] train loss :  0.16 | train acuuracy : 94.03\n",
      "[Epoch: 19] val loss :  0.18 | val acuuracy : 93.52\n",
      "[Epoch: 19] train loss :  0.15 | train acuuracy : 94.17\n",
      "[Epoch: 20] val loss :  0.17 | val acuuracy : 93.63\n",
      "[Epoch: 20] train loss :  0.15 | train acuuracy : 94.27\n",
      "[Epoch: 21] val loss :  0.18 | val acuuracy : 93.65\n",
      "[Epoch: 21] train loss :  0.15 | train acuuracy : 94.32\n",
      "[Epoch: 22] val loss :  0.18 | val acuuracy : 93.53\n",
      "[Epoch: 22] train loss :  0.15 | train acuuracy : 94.44\n",
      "[Epoch: 23] val loss :  0.19 | val acuuracy : 93.23\n",
      "[Epoch: 23] train loss :  0.14 | train acuuracy : 94.62\n",
      "[Epoch: 24] val loss :  0.19 | val acuuracy : 93.42\n",
      "[Epoch: 24] train loss :  0.14 | train acuuracy : 94.71\n",
      "[Epoch: 25] val loss :  0.19 | val acuuracy : 93.43\n",
      "[Epoch: 25] train loss :  0.13 | train acuuracy : 94.93\n",
      "[Epoch: 26] val loss :  0.20 | val acuuracy : 93.26\n",
      "[Epoch: 26] train loss :  0.13 | train acuuracy : 94.97\n",
      "[Epoch: 27] val loss :  0.20 | val acuuracy : 93.12\n",
      "[Epoch: 27] train loss :  0.13 | train acuuracy : 95.18\n",
      "[Epoch: 28] val loss :  0.21 | val acuuracy : 93.22\n",
      "[Epoch: 28] train loss :  0.13 | train acuuracy : 95.25\n",
      "[Epoch: 29] val loss :  0.21 | val acuuracy : 93.20\n",
      "[Epoch: 29] train loss :  0.12 | train acuuracy : 95.42\n",
      "[Epoch: 30] val loss :  0.21 | val acuuracy : 92.73\n",
      "[Epoch: 30] train loss :  0.12 | train acuuracy : 95.43\n",
      "[Epoch: 31] val loss :  0.22 | val acuuracy : 93.16\n",
      "[Epoch: 31] train loss :  0.12 | train acuuracy : 95.70\n",
      "[Epoch: 32] val loss :  0.22 | val acuuracy : 92.85\n",
      "[Epoch: 32] train loss :  0.11 | train acuuracy : 95.78\n",
      "[Epoch: 33] val loss :  0.22 | val acuuracy : 93.18\n",
      "[Epoch: 33] train loss :  0.11 | train acuuracy : 95.91\n",
      "[Epoch: 34] val loss :  0.22 | val acuuracy : 93.08\n",
      "[Epoch: 34] train loss :  0.11 | train acuuracy : 96.06\n",
      "[Epoch: 35] val loss :  0.24 | val acuuracy : 93.05\n",
      "[Epoch: 35] train loss :  0.10 | train acuuracy : 96.19\n",
      "[Epoch: 36] val loss :  0.23 | val acuuracy : 93.05\n",
      "[Epoch: 36] train loss :  0.10 | train acuuracy : 96.22\n",
      "[Epoch: 37] val loss :  0.25 | val acuuracy : 92.92\n",
      "[Epoch: 37] train loss :  0.10 | train acuuracy : 96.34\n",
      "[Epoch: 38] val loss :  0.25 | val acuuracy : 92.97\n",
      "[Epoch: 38] train loss :  0.10 | train acuuracy : 96.46\n",
      "[Epoch: 39] val loss :  0.25 | val acuuracy : 92.87\n",
      "[Epoch: 39] train loss :  0.09 | train acuuracy : 96.53\n",
      "[Epoch: 40] val loss :  0.26 | val acuuracy : 92.62\n",
      "[Epoch: 40] train loss :  0.09 | train acuuracy : 96.59\n",
      "[Epoch: 41] val loss :  0.25 | val acuuracy : 92.84\n",
      "[Epoch: 41] train loss :  0.09 | train acuuracy : 96.81\n",
      "[Epoch: 42] val loss :  0.25 | val acuuracy : 92.76\n",
      "[Epoch: 42] train loss :  0.09 | train acuuracy : 96.78\n",
      "[Epoch: 43] val loss :  0.27 | val acuuracy : 92.83\n",
      "[Epoch: 43] train loss :  0.08 | train acuuracy : 96.88\n",
      "[Epoch: 44] val loss :  0.27 | val acuuracy : 92.92\n",
      "[Epoch: 44] train loss :  0.08 | train acuuracy : 96.87\n",
      "[Epoch: 45] val loss :  0.28 | val acuuracy : 92.33\n",
      "[Epoch: 45] train loss :  0.08 | train acuuracy : 97.03\n",
      "[Epoch: 46] val loss :  0.27 | val acuuracy : 92.76\n",
      "[Epoch: 46] train loss :  0.08 | train acuuracy : 97.03\n",
      "[Epoch: 47] val loss :  0.30 | val acuuracy : 92.87\n",
      "[Epoch: 47] train loss :  0.08 | train acuuracy : 97.18\n",
      "[Epoch: 48] val loss :  0.29 | val acuuracy : 92.80\n",
      "[Epoch: 48] train loss :  0.08 | train acuuracy : 97.14\n",
      "[Epoch: 49] val loss :  0.28 | val acuuracy : 93.00\n",
      "[Epoch: 49] train loss :  0.08 | train acuuracy : 97.22\n",
      "[Epoch: 50] val loss :  0.30 | val acuuracy : 92.73\n",
      "[Epoch: 50] train loss :  0.07 | train acuuracy : 97.32\n",
      "[Epoch: 51] val loss :  0.29 | val acuuracy : 92.70\n",
      "[Epoch: 51] train loss :  0.07 | train acuuracy : 97.38\n",
      "[Epoch: 52] val loss :  0.29 | val acuuracy : 92.60\n",
      "[Epoch: 52] train loss :  0.07 | train acuuracy : 97.43\n",
      "[Epoch: 53] val loss :  0.29 | val acuuracy : 92.58\n",
      "[Epoch: 53] train loss :  0.07 | train acuuracy : 97.48\n",
      "[Epoch: 54] val loss :  0.29 | val acuuracy : 92.77\n",
      "[Epoch: 54] train loss :  0.07 | train acuuracy : 97.51\n",
      "[Epoch: 55] val loss :  0.30 | val acuuracy : 92.72\n",
      "[Epoch: 55] train loss :  0.07 | train acuuracy : 97.58\n",
      "[Epoch: 56] val loss :  0.31 | val acuuracy : 92.77\n",
      "[Epoch: 56] train loss :  0.06 | train acuuracy : 97.60\n",
      "[Epoch: 57] val loss :  0.29 | val acuuracy : 92.79\n",
      "[Epoch: 57] train loss :  0.07 | train acuuracy : 97.58\n",
      "[Epoch: 58] val loss :  0.31 | val acuuracy : 92.87\n",
      "[Epoch: 58] train loss :  0.06 | train acuuracy : 97.64\n",
      "[Epoch: 59] val loss :  0.30 | val acuuracy : 92.68\n",
      "[Epoch: 59] train loss :  0.06 | train acuuracy : 97.72\n",
      "[Epoch: 60] val loss :  0.30 | val acuuracy : 92.90\n",
      "[Epoch: 60] train loss :  0.06 | train acuuracy : 97.72\n",
      "[Epoch: 61] val loss :  0.31 | val acuuracy : 92.72\n",
      "[Epoch: 61] train loss :  0.06 | train acuuracy : 97.81\n",
      "[Epoch: 62] val loss :  0.32 | val acuuracy : 92.99\n",
      "[Epoch: 62] train loss :  0.06 | train acuuracy : 97.81\n",
      "[Epoch: 63] val loss :  0.32 | val acuuracy : 92.86\n",
      "[Epoch: 63] train loss :  0.06 | train acuuracy : 97.88\n",
      "[Epoch: 64] val loss :  0.32 | val acuuracy : 92.82\n",
      "[Epoch: 64] train loss :  0.06 | train acuuracy : 97.92\n",
      "[Epoch: 65] val loss :  0.33 | val acuuracy : 92.40\n",
      "[Epoch: 65] train loss :  0.06 | train acuuracy : 97.95\n",
      "[Epoch: 66] val loss :  0.33 | val acuuracy : 92.60\n",
      "[Epoch: 66] train loss :  0.06 | train acuuracy : 97.94\n",
      "[Epoch: 67] val loss :  0.33 | val acuuracy : 92.64\n",
      "[Epoch: 67] train loss :  0.05 | train acuuracy : 98.00\n",
      "[Epoch: 68] val loss :  0.34 | val acuuracy : 92.63\n",
      "[Epoch: 68] train loss :  0.05 | train acuuracy : 98.04\n",
      "[Epoch: 69] val loss :  0.32 | val acuuracy : 92.60\n",
      "[Epoch: 69] train loss :  0.05 | train acuuracy : 98.08\n",
      "[Epoch: 70] val loss :  0.34 | val acuuracy : 92.68\n",
      "[Epoch: 70] train loss :  0.05 | train acuuracy : 98.09\n",
      "[Epoch: 71] val loss :  0.33 | val acuuracy : 92.72\n",
      "[Epoch: 71] train loss :  0.05 | train acuuracy : 98.06\n",
      "[Epoch: 72] val loss :  0.31 | val acuuracy : 92.75\n",
      "[Epoch: 72] train loss :  0.05 | train acuuracy : 98.17\n",
      "[Epoch: 73] val loss :  0.34 | val acuuracy : 92.63\n",
      "[Epoch: 73] train loss :  0.05 | train acuuracy : 98.16\n",
      "[Epoch: 74] val loss :  0.35 | val acuuracy : 92.57\n",
      "[Epoch: 74] train loss :  0.05 | train acuuracy : 98.20\n",
      "[Epoch: 75] val loss :  0.34 | val acuuracy : 93.01\n",
      "[Epoch: 75] train loss :  0.05 | train acuuracy : 98.19\n",
      "[Epoch: 76] val loss :  0.35 | val acuuracy : 92.94\n",
      "[Epoch: 76] train loss :  0.05 | train acuuracy : 98.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 77] val loss :  0.33 | val acuuracy : 92.57\n",
      "[Epoch: 77] train loss :  0.05 | train acuuracy : 98.25\n",
      "[Epoch: 78] val loss :  0.34 | val acuuracy : 92.88\n",
      "[Epoch: 78] train loss :  0.05 | train acuuracy : 98.25\n",
      "[Epoch: 79] val loss :  0.36 | val acuuracy : 92.65\n",
      "[Epoch: 79] train loss :  0.05 | train acuuracy : 98.25\n",
      "[Epoch: 80] val loss :  0.36 | val acuuracy : 92.70\n",
      "[Epoch: 80] train loss :  0.05 | train acuuracy : 98.33\n",
      "[Epoch: 81] val loss :  0.35 | val acuuracy : 92.64\n",
      "[Epoch: 81] train loss :  0.05 | train acuuracy : 98.32\n",
      "[Epoch: 82] val loss :  0.35 | val acuuracy : 92.54\n",
      "[Epoch: 82] train loss :  0.05 | train acuuracy : 98.32\n",
      "[Epoch: 83] val loss :  0.34 | val acuuracy : 92.38\n",
      "[Epoch: 83] train loss :  0.05 | train acuuracy : 98.34\n",
      "[Epoch: 84] val loss :  0.34 | val acuuracy : 92.79\n",
      "[Epoch: 84] train loss :  0.04 | train acuuracy : 98.35\n",
      "[Epoch: 85] val loss :  0.36 | val acuuracy : 92.74\n",
      "[Epoch: 85] train loss :  0.04 | train acuuracy : 98.45\n",
      "[Epoch: 86] val loss :  0.35 | val acuuracy : 92.64\n",
      "[Epoch: 86] train loss :  0.04 | train acuuracy : 98.40\n",
      "[Epoch: 87] val loss :  0.38 | val acuuracy : 92.45\n",
      "[Epoch: 87] train loss :  0.04 | train acuuracy : 98.43\n",
      "[Epoch: 88] val loss :  0.36 | val acuuracy : 92.68\n",
      "[Epoch: 88] train loss :  0.04 | train acuuracy : 98.44\n",
      "[Epoch: 89] val loss :  0.36 | val acuuracy : 92.78\n",
      "[Epoch: 89] train loss :  0.04 | train acuuracy : 98.47\n",
      "[Epoch: 90] val loss :  0.36 | val acuuracy : 92.66\n",
      "[Epoch: 90] train loss :  0.04 | train acuuracy : 98.45\n",
      "[Epoch: 91] val loss :  0.36 | val acuuracy : 92.64\n",
      "[Epoch: 91] train loss :  0.04 | train acuuracy : 98.48\n",
      "[Epoch: 92] val loss :  0.36 | val acuuracy : 92.70\n",
      "[Epoch: 92] train loss :  0.04 | train acuuracy : 98.51\n",
      "[Epoch: 93] val loss :  0.38 | val acuuracy : 92.70\n",
      "[Epoch: 93] train loss :  0.04 | train acuuracy : 98.53\n",
      "[Epoch: 94] val loss :  0.37 | val acuuracy : 92.60\n",
      "[Epoch: 94] train loss :  0.04 | train acuuracy : 98.57\n",
      "[Epoch: 95] val loss :  0.34 | val acuuracy : 92.58\n",
      "[Epoch: 95] train loss :  0.04 | train acuuracy : 98.51\n",
      "[Epoch: 96] val loss :  0.37 | val acuuracy : 92.70\n",
      "[Epoch: 96] train loss :  0.04 | train acuuracy : 98.62\n",
      "[Epoch: 97] val loss :  0.39 | val acuuracy : 92.66\n",
      "[Epoch: 97] train loss :  0.04 | train acuuracy : 98.57\n",
      "[Epoch: 98] val loss :  0.37 | val acuuracy : 92.54\n",
      "[Epoch: 98] train loss :  0.04 | train acuuracy : 98.52\n",
      "[Epoch: 99] val loss :  0.37 | val acuuracy : 92.64\n",
      "[Epoch: 99] train loss :  0.04 | train acuuracy : 98.59\n",
      "[Epoch: 100] val loss :  0.37 | val acuuracy : 93.02\n",
      "[Epoch: 100] train loss :  0.04 | train acuuracy : 98.60\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs+1):\n",
    "    train_loss, train_accuracy = train(model, optimizer, trainloader)\n",
    "    val_loss, val_accuracy = evaluate(model, valloader)\n",
    "    \n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val acuuracy : %5.2f\" % (epoch, val_loss, val_accuracy))\n",
    "    print(\"[Epoch: %d] train loss : %5.2f | train acuuracy : %5.2f\" % (epoch, train_loss, train_accuracy))\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"./textclassificatior.pt\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c786120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./textclassificatior.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d2c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(88.9446, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, testloader)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13e780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
