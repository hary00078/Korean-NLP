{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fe50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a91cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c94694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./naver_train.csv\", encoding=\"utf-8-sig\")\n",
    "df_test = pd.read_csv(\"./naver_test.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e1521f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터 보고 초딩 영화 줄오버 연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무 재밓었다 그래서 보는 것을 추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다 평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬 페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어 보이기만 했던 커스...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                     흠포스터 보고 초딩 영화 줄오버 연기조차 가볍지 않구나      1\n",
       "2  10265843                             너무 재밓었다 그래서 보는 것을 추천한다      0\n",
       "3   9045019                         교도소 이야기구먼 솔직히 재미는 없다 평점 조정      0\n",
       "4   6483659  사이몬 페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어 보이기만 했던 커스...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78957f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_null(data):\n",
    "    data = data.dropna(axis=0).reset_index(drop=True)\n",
    "    space_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if str.isspace(data.iloc[i, 1]) == True:\n",
    "            space_idx.append(i)\n",
    "    data = data.drop(space_idx)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f384800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = map(delete_null, [df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab831662",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.array(df_train.drop([\"id\"], axis = 1))\n",
    "testset = np.array(df_test.drop([\"id\"], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b274a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset= train_test_split(trainset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eccea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainset[:, 0]\n",
    "y_train = trainset[:, 1]\n",
    "X_val = valset[:, 0]\n",
    "y_val = valset[:, 1]\n",
    "X_test = testset[:, 0]\n",
    "y_test = testset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d7428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(np.int64)\n",
    "y_val = y_val.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d168b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19b8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mecab(\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    return m.morphs(text)\n",
    "\n",
    "def build_vocab(data, threshold):\n",
    "    counter = Counter()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        tokens = tokenizer(data[i])\n",
    "        counter.update(tokens)\n",
    "\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<unk>')\n",
    "    for w in words:\n",
    "        vocab.add_word(w)\n",
    "    return vocab\n",
    "\n",
    "def tokenizing(data, max_length=256):\n",
    "    data_size = len(data)\n",
    "    length = []\n",
    "    for i in range(data_size):\n",
    "        data[i] = tokenizer(data[i])\n",
    "        length.append(len(data[i]))\n",
    "        if len(data[i]) > max_length:\n",
    "            data[i] = data[i, :max_length]\n",
    "        else:\n",
    "            for _ in range(max_length-len(data[i])):\n",
    "                data[i].append(\"<pad>\")\n",
    "    return data, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2a86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(X_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29337879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<pad>', 0), ('<unk>', 1), ('진짜', 2), ('회', 3), ('때', 4), ('부터', 5), ('뀰잼', 6), ('으루', 7), ('보', 8), ('구', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a30327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = map(tokenizing, [X_train, X_val, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddeb90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['진짜', '회', '때', '부터', '뀰잼', '으루', '보', '구', '있', '어', '욧', 'ㅋㅋ', '못', '보', '면', '다운', '받', '아서', '까지', '챙겨', '보', '는', '애청자', '입', '니', '당', 'ㅋㅋ', '빨리', '채연', '이랑', '이', '강준', '한', '민혁', '그', '엄마', '까지', '싹', '다', '태희', '군', '과', '사라', '양', '이', '통쾌', '한', '복수', '할', '수', '있', '도록', '잘', '만들', '어', '주', '세용', '♡', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e78acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e5f6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2idx(data, vocab):\n",
    "    data_size = len(data[0])\n",
    "    sentence_length = len(data[0][0])\n",
    "    for i in range(data_size):\n",
    "        data[0][i] = [vocab(x) for x in data[0][i]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66949295",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = map(lambda data : token2idx(data, vocab), [X_train, X_val, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09d19bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 8, 15, 16, 17, 18, 19, 20, 8, 21, 22, 23, 24, 25, 13, 26, 27, 28, 29, 30, 31, 32, 33, 34, 19, 35, 36, 37, 38, 39, 40, 41, 29, 42, 31, 43, 44, 45, 10, 46, 47, 48, 11, 49, 50, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0be6380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae6e8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, y,):\n",
    "        self.x = data[0]\n",
    "        self.length = data[1]\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.x[index]), self.length[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e0c877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train, y_train)\n",
    "valset = CustomDataset(X_val, y_val)\n",
    "testset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59f0d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  8, 15, 16, 17, 18,\n",
       "         19, 20,  8, 21, 22, 23, 24, 25, 13, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "         19, 35, 36, 37, 38, 39, 40, 41, 29, 42, 31, 43, 44, 45, 10, 46, 47, 48,\n",
       "         11, 49, 50, 51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]),\n",
       " 58,\n",
       " 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0aa46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca376a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 와 cuda 중 다음 기기로 학슴함:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu 와 cuda 중 다음 기기로 학슴함: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b16d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"glove.txt\"\n",
    "output_file = \"tmp.txt\"\n",
    "\n",
    "glove2word2vec(input_file, output_file)\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b3bf226",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.word2idx.keys())\n",
    "embedding_size = 100\n",
    "embedding_weight = np.zeros((vocab_size, embedding_size))\n",
    "for i in range(2, vocab_size):\n",
    "    if vocab.idx2word[i] in glove.key_to_index.keys():\n",
    "        embedding_weight[i] = glove[vocab.idx2word[i]]\n",
    "embedding_weight = torch.tensor(embedding_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "805b4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1209,  0.5049,  0.2698,  ..., -0.1276,  1.5774, -0.5342],\n",
      "        ...,\n",
      "        [ 0.0136, -0.0377,  0.3660,  ..., -0.3099,  0.8662,  0.0130],\n",
      "        [-0.2016, -0.3475, -0.2892,  ..., -0.6292,  0.3447,  0.6372],\n",
      "        [-0.1395,  0.1505, -0.0912,  ..., -0.0351, -0.0776,  0.7689]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4b5ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first= True,  bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_dim*2, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        embeded = self.embed(x)\n",
    "        packed_input = pack_padded_sequence(embeded, length.tolist(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output,(hidden, cell) = self.lstm(packed_input)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        logit = self.out(hidden)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b90d1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "model = LSTM(3, 256, vocab_size, 100, n_classes).to(DEVICE)\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deab5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x , l, y = batch\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        y = y.reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x, l)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92eab1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x , l, y = batch\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.long().to(DEVICE)\n",
    "            y = y.reshape(-1)\n",
    "            logit = model(x, l)\n",
    "            loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()    \n",
    "            size += x.shape[0]\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3321f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cdd5971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1209,  0.5049,  0.2698,  ..., -0.1276,  1.5774, -0.5342],\n",
       "        ...,\n",
       "        [ 0.0136, -0.0377,  0.3660,  ..., -0.3099,  0.8662,  0.0130],\n",
       "        [-0.2016, -0.3475, -0.2892,  ..., -0.6292,  0.3447,  0.6372],\n",
       "        [-0.1395,  0.1505, -0.0912,  ..., -0.0351, -0.0776,  0.7689]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.data.copy_(embedding_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf1a3fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.432 | Train Acc: 79.82%\n",
      "\t Val. Loss: 0.387 |  Val. Acc: 81.75%\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.363 | Train Acc: 83.63%\n",
      "\t Val. Loss: 0.355 |  Val. Acc: 84.32%\n",
      "Epoch: 03 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.331 | Train Acc: 85.45%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 82.99%\n",
      "Epoch: 04 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.308 | Train Acc: 86.80%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 85.63%\n",
      "Epoch: 05 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.288 | Train Acc: 87.75%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 85.95%\n",
      "Epoch: 06 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.270 | Train Acc: 88.73%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 86.33%\n",
      "Epoch: 07 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.253 | Train Acc: 89.64%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 86.27%\n",
      "Epoch: 08 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.237 | Train Acc: 90.42%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 86.41%\n",
      "Epoch: 09 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.14%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 86.22%\n",
      "Epoch: 10 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.207 | Train Acc: 91.83%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 85.92%\n",
      "Epoch: 11 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.192 | Train Acc: 92.47%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 86.09%\n",
      "Epoch: 12 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.178 | Train Acc: 93.08%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 85.74%\n",
      "Epoch: 13 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.165 | Train Acc: 93.70%\n",
      "\t Val. Loss: 0.383 |  Val. Acc: 85.52%\n",
      "Epoch: 14 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.153 | Train Acc: 94.16%\n",
      "\t Val. Loss: 0.448 |  Val. Acc: 85.31%\n",
      "Epoch: 15 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.141 | Train Acc: 94.66%\n",
      "\t Val. Loss: 0.461 |  Val. Acc: 85.19%\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "n_epochs = 15\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, optimizer, trainloader)\n",
    "    val_loss, val_accuracy = evaluate(model, valloader)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"./textclassificatior.pt\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b28b397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./textclassificatior.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "266acae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(86.4488, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, testloader)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607681b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
